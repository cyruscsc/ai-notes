## Factuality hallucination detection

### Fact checking

- Extract and verify the correctness of the independent factual statements within the model’s outputs against trusted knowledge sources

#### External retrieval 

- Decompose the generation content into atomic facts
- Compute the percentage supported by reliable knowledge sources

#### Internal checking

- Generate verification questions for a draft response
- Leverage [[Large Language Models|LLMs]]' parametric knowledge to assess the consistency of the answer against the original response
- As [[Large Language Models|LLMs]] are not inherently reliable factual databases, solely relying on LLMs’ parametric knowledge for fact-checking may result in inaccurate assessments

### Uncertainty estimation

- Detect [[hallucination]] by estimating the uncertainty of the factual content generated by the model
- Address the issue in zero-resource settings, eliminating the need for retrieval

#### LLM internal states

- The internal states of [[Large Language Models|LLMs]] can serve as informative indicators of their uncertainty
- Often manifested through metrics like token probability or entropy

#### LLM behaviour

- Probe a model’s uncertainty through natural language prompts or by examining its behavioural manifestations
- Uncertainty can be assessed from the self-consistency of a single LLM’s multiple generations
- May also embrace a multi-agent perspective by incorporating additional [[Large Language Models|LLMs]]

## Faithfulness hallucination detection

### Fact-based metrics

- Measure the overlap of pivotal facts between the generated content and the source content
- Can base on [[n-grams]], entities, or relation tuples

### Classifier-based metrics

- Utilize classifiers trained on data from related tasks, such as natural language inference (NLI) and fact-checking, or data comprised of synthetically task-specific hallucinated and faithful content
- Genuinely faithful content should inherently be entailed by its source content

### QA-based metrics

- Enhanced ability to capture information overlap between the model’s generation and its source
- Initially select target answers from the information units within the LLM’s output 
- Then questions are generated by the question-generation module
- The questions are subsequently used to generate source answers based on the user context
- Finally, the faithfulness of the LLM’s responses is calculated by comparing the matching scores between the source and target answers

### Uncertainty-based metrics

- Draws parallels with the uncertainty-based approaches employed for detecting [[factuality hallucinations]]
- Can base on entropy or log-probability

### LLM-based judgement

- Assess faithfulness automatically by providing [[Large Language Models|LLMs]] with concrete evaluation guidelines and feeding them both the model-generated and source content
- The final evaluation output can either be a binary or scaled judgment
- Evaluation prompt can either be direct prompting, chain-of-thought prompting
- Use in-context learning or allow the model to generate evaluation results accompanying explanations
