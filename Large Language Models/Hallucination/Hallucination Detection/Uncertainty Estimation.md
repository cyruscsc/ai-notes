## Definition

- Detect [[hallucination]] by estimating the uncertainty of the factual content generated by the model
- Address the issue in zero-resource settings, eliminating the need for [[retrieval]]

## LLM internal states

- The internal states of [[Large Language Models|LLMs]] can serve as informative indicators of their uncertainty
- Often manifested through metrics like token probability or entropy

## LLM behaviour

- Probe a model’s uncertainty through natural language prompts or by examining its behavioural manifestations
- Uncertainty can be assessed from the self-consistency of a single LLM’s multiple generations
- May also embrace a multi-agent perspective by incorporating additional [[Large Language Models|LLMs]]
