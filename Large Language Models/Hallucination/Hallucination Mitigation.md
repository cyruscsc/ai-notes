## Data-related hallucinations

### Data filtering

- Select high-quality [[pre-training]] data from reliable sources carefully
- Ensure the factual correctness of data while also minimizing the introduction of social biases

### Model editing

- Inject up-to-date knowledge by editing model’s parameters
- Rectify model behaviour by incorporating additional knowledge

#### Locate-then-edit

- First locate the "buggy" part of the model parameters
- Then apply an update to them to alter the model’s behaviour

#### Meta-learning

- Train an external hyper-network to predict the weight update of the original model
- Often require additional training and memory cost

### Retrieval augmented generation

- Leverage external non-parametric database for knowledge supplying
- First retrieve relevant knowledge from external sources by a retriever
- Then the final response is generated by a generator conditioning on both user query and retrieved documents

#### One-time retrieval

- Directly prepend the external knowledge obtained from a single retrieval to the [[Large Language Models|LLMs]]’ prompt
- May fall short when confronted with intricate challenges like multi-step reasoning and long-form question answering

#### Iterative retrieval

- Allow for continuously gathering knowledge throughout the generation process
- Incorporate external knowledge at each reasoning step and further guide retrieval process based on ongoing reasoning for chain-of-thought prompting, reducing factual errors in reasoning chains

#### Post-hoc retrieval

- Research relevant evidence and subsequently revise the initial generation based on detected discrepancies with the evidence
- May generate verifying questions and then refines the rationales based on retrieved knowledge, ensuring a more factual response
- May sample various potential answers, allowing for a more comprehensive retrieval feedback

## Training-related hallucinations

### Pre-training related

- Centers on the limitations inherent in model architectures, especially unidirectional representation and [[attention]] glitches
- Refine [[pre-training]] strategies, ensuring richer context comprehension, and circumventing biases
- The choice of objective plays a pivotal role in determining the model’s performance

### Misalignment

- Hallucinations stemming from belief misalignment often manifest as sycophancy, a tendency of LLMs to seek human approval in undesirable ways
- Improve human preference judgments and, by extension, the preference model
- [[Fine-tuning]] or activation steering

## Inference-related hallucinations

### Factuality enhanced decoding

- Improve the reliability of outputs from LLMs by prioritizing the factuality of the information they generate
- Focus on aligning model outputs closely with established real-world facts, thereby minimizing the risk of disseminating false or misleading information

#### Factuality decoding

- Dynamically adjust the nucleus probability throughout sentence generation based on decay factors and lower boundaries
- Adjust activations along the truth-correlated direction during inference
- Dynamically select and contrasts logits from different layers to refine decoding factuality

#### Post-editing decoding

- Harness the self-correction capabilities of [[Large Language Models|LLMs]] to refine the originally generated content without relying on an external knowledge base
- Leverage the inherent ability of [[Large Language Models|LLMs]] to first generate factual knowledge and then refine the response until it aligns consistently with the provided background knowledge

### Faithfulness enhanced decoding

- Prioritize alignment with the provided context and also emphasizes enhancing the consistency within the generated content

#### Context consistency

- Encourage the LLM to focus more on contextual information rather than over-rely on prior knowledge
- Detect contextual hallucinations at different levels and then refine the generated response
- Overcome the [[Softmax Function|softmax]] bottleneck that constrains the expression of diversity and faithful representations

#### Logical consistency

- The unfaithfulness issue lies in the inconsistencies in the context information obtained by the CoT and the answer
- Enhance the self-consistency in chain-of-thought
- Eliminate reasoning shortcuts that derive answers without considering the rationale
- Reduce surface-level copying and prevent missed reasoning steps
- Align the LLM towards preferring correct reasoning chains over counterfactual chains
- Encourage the LLM to reason faithfully over the reasoning steps using a counterfactual and causal preference objective
