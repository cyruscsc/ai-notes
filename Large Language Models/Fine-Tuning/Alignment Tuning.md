## Definition

- A [[fine-tuning]] technique that focuses on aligning the model's outputs with human values, preferences, and ethical considerations

## Characteristics

- Aims to make [[Large Language Models|LLMs]] safer, more reliable, and better aligned with human intentions
- Often involves [[Reinforcement Learning from Human Feedback|RLHF]] to encourage desirable behaviors and discourage harmful or biased outputs
