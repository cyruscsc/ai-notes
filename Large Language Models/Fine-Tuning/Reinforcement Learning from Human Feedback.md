---
aliases:
  - RLHF
---

## Definition

- An advanced [[machine learning]] technique used to align [[Large Language Models|LLMs]] with human preferences and intentions
- Combines [[reinforcement learning]] principles with direct human input to enhance the performance and behavior of AI systems

## Components

- A [[Pre-Training|pre-trained]] model
- Human feedback collection
- Preference model / reward model
- Policy optimization

## Techniques

- **Proximal Policy Optimization (PPO)**: an algorithm used to update the language model's policy while preventing drastic changes
- **Kullback-Leibler (KL) Divergence**: a method to measure and control the difference between the original and fine-tuned model distributions
