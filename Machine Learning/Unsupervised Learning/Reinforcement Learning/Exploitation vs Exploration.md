## Exploitation

- Takes the actions that are already established to bring to good outcomes
- Always follows the same path to the solution
- Never find a better path

## Exploration

- May use a previously unexplored route on its way to the target
- Allows to discover more efficient solutions along the way

## Improving efficiency

### ε greedy algorithm

- Set $\epsilon$ equal to how often we want to move randomly
- With probability $1-\epsilon$, the algorithm chooses the best move (exploitation)
- With probability $\epsilon$, the algorithm chooses a random move (exploration)

> See [[Deep Q-Network#ε-greedy policy|ε-greedy policy]]

### Function approximation

- Approximate $Q(s,a)$ using various other features
- Rather than storing one value for each state-action pair
- The algorithm becomes able to recognize which moves are similar enough so that their estimated value should be similar as well, and use this [[heuristic]] in its decision making